{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29098,
     "status": "ok",
     "timestamp": 1576383508980,
     "user": {
      "displayName": "Кирилл Резников",
      "photoUrl": "",
      "userId": "15959288999881424210"
     },
     "user_tz": -120
    },
    "id": "yffGeFRFYNcv",
    "outputId": "df8140bc-47bb-471c-8ac8-fa01ecc2a79c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1bOi4wwIg5j7"
   },
   "outputs": [],
   "source": [
    "root_path = 'gdrive/My Drive/Colab Notebooks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4249,
     "status": "ok",
     "timestamp": 1576383516967,
     "user": {
      "displayName": "Кирилл Резников",
      "photoUrl": "",
      "userId": "15959288999881424210"
     },
     "user_tz": -120
    },
    "id": "BbEMBxEceH8b",
    "outputId": "31dbd0ee-1c30-4cf5-b64f-7d204a585c63"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Conv2D, MaxPooling2D\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from keras import utils\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kcMXx-SteVFu"
   },
   "outputs": [],
   "source": [
    "X_train = np.load(\"gdrive/My Drive/Colab Notebooks/k49-train-imgs.npz\")['arr_0']\n",
    "y_train = np.load(\"gdrive/My Drive/Colab Notebooks/k49-train-labels.npz\")['arr_0']\n",
    "X_test = np.load(\"gdrive/My Drive/Colab Notebooks/k49-test-imgs.npz\")['arr_0']\n",
    "y_test = np.load(\"gdrive/My Drive/Colab Notebooks/k49-test-labels.npz\")['arr_0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2TqUQFJyjquz"
   },
   "outputs": [],
   "source": [
    "#X_train1 = X_train[:60000]\n",
    "#X_test1 = X_test[:1000]\n",
    "#y_train1 = y_train[:60000]\n",
    "#y_test1 = y_test[:1000]\n",
    "X_train1 = X_train\n",
    "X_test1 = X_test\n",
    "y_train1 = y_train\n",
    "y_test1 = y_test\n",
    "X_train1 = X_train1/255\n",
    "X_test1 = X_test1/255\n",
    "y_train1 = utils.to_categorical(y_train1, 49)\n",
    "y_test1 = utils.to_categorical(y_test1, 49)\n",
    "#для персептронов\n",
    "#X_train1 = np.reshape(X_train1, (X_train1.shape[0], 28*28))\n",
    "#X_test1 = np.reshape(X_test1, (X_test1.shape[0], 28*28))\n",
    "#для сверточной\n",
    "X_train1 = X_train1.reshape(-1, 28,28, 1)\n",
    "X_test1 = X_test1.reshape(-1, 28,28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 626,
     "status": "ok",
     "timestamp": 1576097190070,
     "user": {
      "displayName": "Кирилл Резников",
      "photoUrl": "",
      "userId": "15959288999881424210"
     },
     "user_tz": -120
    },
    "id": "rUWnjxwAYcGO",
    "outputId": "294cb56f-9c72-48c7-f9cd-6990b8326262"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 7000)\n",
      "(1, 7000)\n",
      "(2, 7000)\n",
      "(3, 903)\n",
      "(4, 7000)\n",
      "(5, 7000)\n",
      "(6, 7000)\n",
      "(7, 7000)\n",
      "(8, 5481)\n",
      "(9, 7000)\n",
      "(10, 7000)\n",
      "(11, 7000)\n",
      "(12, 7000)\n",
      "(13, 4843)\n",
      "(14, 4496)\n",
      "(15, 7000)\n",
      "(16, 2983)\n",
      "(17, 7000)\n",
      "(18, 7000)\n",
      "(19, 7000)\n",
      "(20, 7000)\n",
      "(21, 7000)\n",
      "(22, 2399)\n",
      "(23, 2850)\n",
      "(24, 7000)\n",
      "(25, 7000)\n",
      "(26, 5968)\n",
      "(27, 7000)\n",
      "(28, 7000)\n",
      "(29, 2317)\n",
      "(30, 7000)\n",
      "(31, 3558)\n",
      "(32, 1998)\n",
      "(33, 3946)\n",
      "(34, 7000)\n",
      "(35, 7000)\n",
      "(36, 1858)\n",
      "(37, 7000)\n",
      "(38, 7000)\n",
      "(39, 7000)\n",
      "(40, 7000)\n",
      "(41, 7000)\n",
      "(42, 2487)\n",
      "(43, 2787)\n",
      "(44, 485)\n",
      "(45, 456)\n",
      "(46, 7000)\n",
      "(47, 7000)\n",
      "(48, 4097)\n"
     ]
    }
   ],
   "source": [
    "counts = np.bincount(y_train)\n",
    "counts += np.bincount(y_test)\n",
    "# print(counts.shape)\n",
    "# печатаем частоты рядом с ответами \n",
    "for count in enumerate(counts): \n",
    "    print(count) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3281231,
     "status": "ok",
     "timestamp": 1571527472104,
     "user": {
      "displayName": "Кирилл Резников",
      "photoUrl": "",
      "userId": "15959288999881424210"
     },
     "user_tz": -180
    },
    "id": "3MeEUE37edWQ",
    "outputId": "e609aae8-4f99-4310-82e7-07be7d807fcf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.938493 using {'dense_1': 1024, 'dense_2': 1024, 'dense_3': 512, 'dropout_rate_1': 0.5, 'dropout_rate_2': 0.2, 'dropout_rate_3': 0.1}\n",
      "0.935253 (0.000404) with: {'dense_1': 1024, 'dense_2': 1024, 'dense_3': 512, 'dropout_rate_1': 0.3, 'dropout_rate_2': 0.1, 'dropout_rate_3': 0}\n",
      "0.935025 (0.000426) with: {'dense_1': 1024, 'dense_2': 1024, 'dense_3': 512, 'dropout_rate_1': 0.3, 'dropout_rate_2': 0.1, 'dropout_rate_3': 0.1}\n",
      "0.935072 (0.000222) with: {'dense_1': 1024, 'dense_2': 1024, 'dense_3': 512, 'dropout_rate_1': 0.3, 'dropout_rate_2': 0.2, 'dropout_rate_3': 0}\n",
      "0.936264 (0.000983) with: {'dense_1': 1024, 'dense_2': 1024, 'dense_3': 512, 'dropout_rate_1': 0.3, 'dropout_rate_2': 0.2, 'dropout_rate_3': 0.1}\n",
      "0.935971 (0.000932) with: {'dense_1': 1024, 'dense_2': 1024, 'dense_3': 512, 'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'dropout_rate_3': 0}\n",
      "0.936221 (0.000925) with: {'dense_1': 1024, 'dense_2': 1024, 'dense_3': 512, 'dropout_rate_1': 0.3, 'dropout_rate_2': 0.3, 'dropout_rate_3': 0.1}\n",
      "0.936027 (0.000702) with: {'dense_1': 1024, 'dense_2': 1024, 'dense_3': 512, 'dropout_rate_1': 0.4, 'dropout_rate_2': 0.1, 'dropout_rate_3': 0}\n",
      "0.936070 (0.000222) with: {'dense_1': 1024, 'dense_2': 1024, 'dense_3': 512, 'dropout_rate_1': 0.4, 'dropout_rate_2': 0.1, 'dropout_rate_3': 0.1}\n",
      "0.936940 (0.001052) with: {'dense_1': 1024, 'dense_2': 1024, 'dense_3': 512, 'dropout_rate_1': 0.4, 'dropout_rate_2': 0.2, 'dropout_rate_3': 0}\n",
      "0.937555 (0.001138) with: {'dense_1': 1024, 'dense_2': 1024, 'dense_3': 512, 'dropout_rate_1': 0.4, 'dropout_rate_2': 0.2, 'dropout_rate_3': 0.1}\n",
      "0.936957 (0.000861) with: {'dense_1': 1024, 'dense_2': 1024, 'dense_3': 512, 'dropout_rate_1': 0.4, 'dropout_rate_2': 0.3, 'dropout_rate_3': 0}\n",
      "0.937026 (0.000960) with: {'dense_1': 1024, 'dense_2': 1024, 'dense_3': 512, 'dropout_rate_1': 0.4, 'dropout_rate_2': 0.3, 'dropout_rate_3': 0.1}\n",
      "0.936707 (0.000319) with: {'dense_1': 1024, 'dense_2': 1024, 'dense_3': 512, 'dropout_rate_1': 0.5, 'dropout_rate_2': 0.1, 'dropout_rate_3': 0}\n",
      "0.937443 (0.000081) with: {'dense_1': 1024, 'dense_2': 1024, 'dense_3': 512, 'dropout_rate_1': 0.5, 'dropout_rate_2': 0.1, 'dropout_rate_3': 0.1}\n",
      "0.937559 (0.000440) with: {'dense_1': 1024, 'dense_2': 1024, 'dense_3': 512, 'dropout_rate_1': 0.5, 'dropout_rate_2': 0.2, 'dropout_rate_3': 0}\n",
      "0.938493 (0.000651) with: {'dense_1': 1024, 'dense_2': 1024, 'dense_3': 512, 'dropout_rate_1': 0.5, 'dropout_rate_2': 0.2, 'dropout_rate_3': 0.1}\n",
      "0.936832 (0.000934) with: {'dense_1': 1024, 'dense_2': 1024, 'dense_3': 512, 'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'dropout_rate_3': 0}\n",
      "0.937456 (0.000587) with: {'dense_1': 1024, 'dense_2': 1024, 'dense_3': 512, 'dropout_rate_1': 0.5, 'dropout_rate_2': 0.3, 'dropout_rate_3': 0.1}\n"
     ]
    }
   ],
   "source": [
    "def create_model(dropout_rate_1=0, dropout_rate_2=0, dropout_rate_3=0,\n",
    "                 dense_1=1024, dense_2=512, dense_3=256):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dense_1, input_dim=784, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate_1))\n",
    "    model.add(Dense(dense_2, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate_2))\n",
    "    model.add(Dense(dense_3, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate_3))\n",
    "    model.add(Dense(49, activation = 'softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=20, batch_size=512, verbose=0)\n",
    "\n",
    "dropout_rate_1 = [0.3, 0.4, 0.5]\n",
    "dropout_rate_2 = [0.1, 0.2, 0.3]\n",
    "dropout_rate_3 = [0, 0.1]\n",
    "dense_1 = [1024]\n",
    "dense_2 = [1024]\n",
    "dense_3 = [512]\n",
    "param_grid = dict(dropout_rate_1=dropout_rate_1, dropout_rate_2=dropout_rate_2,\n",
    "                 dropout_rate_3=dropout_rate_3, dense_1=dense_1,\n",
    "                 dense_2=dense_2, dense_3=dense_3)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train1, y_train1)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 142808,
     "status": "ok",
     "timestamp": 1576336440972,
     "user": {
      "displayName": "Кирилл Резников",
      "photoUrl": "",
      "userId": "15959288999881424210"
     },
     "user_tz": -120
    },
    "id": "PncGrMeTFSxH",
    "outputId": "4421dd1e-43f3-4c79-cf7a-11d2cbfcb55e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 185892 samples, validate on 46473 samples\n",
      "Epoch 1/75\n",
      "185892/185892 [==============================] - 2s 11us/step - loss: 1.1452 - acc: 0.7111 - val_loss: 0.6742 - val_acc: 0.8267\n",
      "Epoch 2/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.6328 - acc: 0.8329 - val_loss: 0.4938 - val_acc: 0.8712\n",
      "Epoch 3/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.5037 - acc: 0.8646 - val_loss: 0.4167 - val_acc: 0.8905\n",
      "Epoch 4/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.4298 - acc: 0.8830 - val_loss: 0.3764 - val_acc: 0.8999\n",
      "Epoch 5/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.3837 - acc: 0.8953 - val_loss: 0.3486 - val_acc: 0.9066\n",
      "Epoch 6/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.3460 - acc: 0.9040 - val_loss: 0.3292 - val_acc: 0.9120\n",
      "Epoch 7/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.3202 - acc: 0.9103 - val_loss: 0.3200 - val_acc: 0.9140\n",
      "Epoch 8/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.2938 - acc: 0.9170 - val_loss: 0.3099 - val_acc: 0.9175\n",
      "Epoch 9/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.2745 - acc: 0.9217 - val_loss: 0.2979 - val_acc: 0.9197\n",
      "Epoch 10/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.2571 - acc: 0.9270 - val_loss: 0.2938 - val_acc: 0.9223\n",
      "Epoch 11/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.2429 - acc: 0.9296 - val_loss: 0.2916 - val_acc: 0.9226\n",
      "Epoch 12/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.2314 - acc: 0.9323 - val_loss: 0.2872 - val_acc: 0.9233\n",
      "Epoch 13/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.2209 - acc: 0.9350 - val_loss: 0.2850 - val_acc: 0.9241\n",
      "Epoch 14/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.2072 - acc: 0.9384 - val_loss: 0.2845 - val_acc: 0.9239\n",
      "Epoch 15/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1998 - acc: 0.9403 - val_loss: 0.2842 - val_acc: 0.9257\n",
      "Epoch 16/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1912 - acc: 0.9429 - val_loss: 0.2791 - val_acc: 0.9270\n",
      "Epoch 17/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1804 - acc: 0.9453 - val_loss: 0.2778 - val_acc: 0.9279\n",
      "Epoch 18/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1767 - acc: 0.9461 - val_loss: 0.2826 - val_acc: 0.9274\n",
      "Epoch 19/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1686 - acc: 0.9484 - val_loss: 0.2811 - val_acc: 0.9277\n",
      "Epoch 20/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1626 - acc: 0.9496 - val_loss: 0.2854 - val_acc: 0.9277\n",
      "Epoch 21/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1579 - acc: 0.9512 - val_loss: 0.2826 - val_acc: 0.9286\n",
      "Epoch 22/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1515 - acc: 0.9531 - val_loss: 0.2881 - val_acc: 0.9283\n",
      "Epoch 23/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1480 - acc: 0.9536 - val_loss: 0.2881 - val_acc: 0.9286\n",
      "Epoch 24/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1421 - acc: 0.9555 - val_loss: 0.2877 - val_acc: 0.9292\n",
      "Epoch 25/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1386 - acc: 0.9561 - val_loss: 0.2957 - val_acc: 0.9284\n",
      "Epoch 26/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1336 - acc: 0.9577 - val_loss: 0.2927 - val_acc: 0.9289\n",
      "Epoch 27/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1312 - acc: 0.9588 - val_loss: 0.2921 - val_acc: 0.9296\n",
      "Epoch 28/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1319 - acc: 0.9583 - val_loss: 0.3002 - val_acc: 0.9284\n",
      "Epoch 29/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1246 - acc: 0.9603 - val_loss: 0.2987 - val_acc: 0.9298\n",
      "Epoch 30/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1229 - acc: 0.9607 - val_loss: 0.3030 - val_acc: 0.9283\n",
      "Epoch 31/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1177 - acc: 0.9623 - val_loss: 0.3011 - val_acc: 0.9304\n",
      "Epoch 32/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1166 - acc: 0.9623 - val_loss: 0.3026 - val_acc: 0.9294\n",
      "Epoch 33/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1117 - acc: 0.9641 - val_loss: 0.3029 - val_acc: 0.9300\n",
      "Epoch 34/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1102 - acc: 0.9644 - val_loss: 0.3031 - val_acc: 0.9307\n",
      "Epoch 35/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1095 - acc: 0.9645 - val_loss: 0.3100 - val_acc: 0.9291\n",
      "Epoch 36/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1072 - acc: 0.9652 - val_loss: 0.3094 - val_acc: 0.9301\n",
      "Epoch 37/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1045 - acc: 0.9662 - val_loss: 0.3097 - val_acc: 0.9311\n",
      "Epoch 38/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1031 - acc: 0.9664 - val_loss: 0.3187 - val_acc: 0.9294\n",
      "Epoch 39/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.1011 - acc: 0.9666 - val_loss: 0.3176 - val_acc: 0.9297\n",
      "Epoch 40/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0968 - acc: 0.9681 - val_loss: 0.3211 - val_acc: 0.9301\n",
      "Epoch 41/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0977 - acc: 0.9678 - val_loss: 0.3178 - val_acc: 0.9312\n",
      "Epoch 42/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0954 - acc: 0.9687 - val_loss: 0.3189 - val_acc: 0.9301\n",
      "Epoch 43/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0921 - acc: 0.9700 - val_loss: 0.3211 - val_acc: 0.9296\n",
      "Epoch 44/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0949 - acc: 0.9691 - val_loss: 0.3250 - val_acc: 0.9296\n",
      "Epoch 45/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0896 - acc: 0.9707 - val_loss: 0.3263 - val_acc: 0.9309\n",
      "Epoch 46/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0913 - acc: 0.9701 - val_loss: 0.3247 - val_acc: 0.9306\n",
      "Epoch 47/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0904 - acc: 0.9703 - val_loss: 0.3275 - val_acc: 0.9303\n",
      "Epoch 48/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0897 - acc: 0.9704 - val_loss: 0.3307 - val_acc: 0.9300\n",
      "Epoch 49/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0872 - acc: 0.9711 - val_loss: 0.3324 - val_acc: 0.9300\n",
      "Epoch 50/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0845 - acc: 0.9721 - val_loss: 0.3364 - val_acc: 0.9310\n",
      "Epoch 51/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0836 - acc: 0.9723 - val_loss: 0.3403 - val_acc: 0.9297\n",
      "Epoch 52/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0821 - acc: 0.9727 - val_loss: 0.3435 - val_acc: 0.9300\n",
      "Epoch 53/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0812 - acc: 0.9730 - val_loss: 0.3410 - val_acc: 0.9304\n",
      "Epoch 54/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0828 - acc: 0.9727 - val_loss: 0.3445 - val_acc: 0.9297\n",
      "Epoch 55/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0789 - acc: 0.9741 - val_loss: 0.3470 - val_acc: 0.9301\n",
      "Epoch 56/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0771 - acc: 0.9737 - val_loss: 0.3531 - val_acc: 0.9291\n",
      "Epoch 57/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0788 - acc: 0.9740 - val_loss: 0.3464 - val_acc: 0.9301\n",
      "Epoch 58/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0769 - acc: 0.9746 - val_loss: 0.3537 - val_acc: 0.9296\n",
      "Epoch 59/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0754 - acc: 0.9745 - val_loss: 0.3542 - val_acc: 0.9293\n",
      "Epoch 60/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0794 - acc: 0.9739 - val_loss: 0.3566 - val_acc: 0.9291\n",
      "Epoch 61/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0736 - acc: 0.9754 - val_loss: 0.3600 - val_acc: 0.9291\n",
      "Epoch 62/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0747 - acc: 0.9749 - val_loss: 0.3634 - val_acc: 0.9294\n",
      "Epoch 63/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0737 - acc: 0.9754 - val_loss: 0.3588 - val_acc: 0.9305\n",
      "Epoch 64/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0716 - acc: 0.9762 - val_loss: 0.3595 - val_acc: 0.9299\n",
      "Epoch 65/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0735 - acc: 0.9756 - val_loss: 0.3641 - val_acc: 0.9299\n",
      "Epoch 66/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0728 - acc: 0.9759 - val_loss: 0.3657 - val_acc: 0.9292\n",
      "Epoch 67/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0708 - acc: 0.9761 - val_loss: 0.3712 - val_acc: 0.9292\n",
      "Epoch 68/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0734 - acc: 0.9755 - val_loss: 0.3716 - val_acc: 0.9305\n",
      "Epoch 69/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0674 - acc: 0.9776 - val_loss: 0.3729 - val_acc: 0.9300\n",
      "Epoch 70/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0691 - acc: 0.9771 - val_loss: 0.3739 - val_acc: 0.9294\n",
      "Epoch 71/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0684 - acc: 0.9771 - val_loss: 0.3715 - val_acc: 0.9310\n",
      "Epoch 72/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0677 - acc: 0.9776 - val_loss: 0.3705 - val_acc: 0.9305\n",
      "Epoch 73/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0689 - acc: 0.9773 - val_loss: 0.3754 - val_acc: 0.9298\n",
      "Epoch 74/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0667 - acc: 0.9778 - val_loss: 0.3778 - val_acc: 0.9302\n",
      "Epoch 75/75\n",
      "185892/185892 [==============================] - 2s 10us/step - loss: 0.0673 - acc: 0.9774 - val_loss: 0.3784 - val_acc: 0.9291\n",
      "38547/38547 [==============================] - 1s 30us/step\n",
      "0.868576024066775\n"
     ]
    }
   ],
   "source": [
    "#1 слой\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=784, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(49, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train1, y_train1, batch_size=512, epochs=75, validation_split=0.2, verbose=1)\n",
    "test_loss, test_acc = model.evaluate(X_test1, y_test1)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFQ9rQoUlNWL"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 786,
     "status": "ok",
     "timestamp": 1576347666175,
     "user": {
      "displayName": "Кирилл Резников",
      "photoUrl": "",
      "userId": "15959288999881424210"
     },
     "user_tz": -120
    },
    "id": "YxeNKvGbkyqu",
    "outputId": "89b560be-df40-4c58-edcd-37287ccf0df0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9stPougTYgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.plot(history.history['acc'], \n",
    "         label='Доля верных ответов на обучающем наборе')\n",
    "plt.plot(history.history['val_acc'], \n",
    "         label='Доля верных ответов на проверочном наборе')\n",
    "plt.xlabel('Эпоха обучения')\n",
    "plt.ylabel('Доля верных ответов')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 216131,
     "status": "ok",
     "timestamp": 1576341698488,
     "user": {
      "displayName": "Кирилл Резников",
      "photoUrl": "",
      "userId": "15959288999881424210"
     },
     "user_tz": -120
    },
    "id": "U99Xby3FFVWN",
    "outputId": "ff37e6dc-f4ef-4d85-aeeb-feda740c7758"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 185892 samples, validate on 46473 samples\n",
      "Epoch 1/75\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "185892/185892 [==============================] - 7s 40us/step - loss: 1.1321 - acc: 0.7027 - val_loss: 0.5760 - val_acc: 0.8439\n",
      "Epoch 2/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.6545 - acc: 0.8200 - val_loss: 0.4337 - val_acc: 0.8809\n",
      "Epoch 3/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.5356 - acc: 0.8507 - val_loss: 0.3650 - val_acc: 0.9012\n",
      "Epoch 4/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.4657 - acc: 0.8680 - val_loss: 0.3290 - val_acc: 0.9083\n",
      "Epoch 5/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.4174 - acc: 0.8802 - val_loss: 0.3061 - val_acc: 0.9160\n",
      "Epoch 6/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.3826 - acc: 0.8899 - val_loss: 0.2898 - val_acc: 0.9196\n",
      "Epoch 7/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.3600 - acc: 0.8959 - val_loss: 0.2749 - val_acc: 0.9241\n",
      "Epoch 8/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.3368 - acc: 0.9006 - val_loss: 0.2649 - val_acc: 0.9272\n",
      "Epoch 9/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.3205 - acc: 0.9053 - val_loss: 0.2571 - val_acc: 0.9292\n",
      "Epoch 10/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.3034 - acc: 0.9098 - val_loss: 0.2542 - val_acc: 0.9285\n",
      "Epoch 11/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.2921 - acc: 0.9131 - val_loss: 0.2439 - val_acc: 0.9323\n",
      "Epoch 12/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.2823 - acc: 0.9152 - val_loss: 0.2474 - val_acc: 0.9311\n",
      "Epoch 13/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.2701 - acc: 0.9187 - val_loss: 0.2380 - val_acc: 0.9347\n",
      "Epoch 14/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.2585 - acc: 0.9218 - val_loss: 0.2364 - val_acc: 0.9355\n",
      "Epoch 15/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.2520 - acc: 0.9234 - val_loss: 0.2310 - val_acc: 0.9366\n",
      "Epoch 16/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.2466 - acc: 0.9251 - val_loss: 0.2335 - val_acc: 0.9361\n",
      "Epoch 17/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.2429 - acc: 0.9259 - val_loss: 0.2265 - val_acc: 0.9381\n",
      "Epoch 18/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.2356 - acc: 0.9275 - val_loss: 0.2303 - val_acc: 0.9365\n",
      "Epoch 19/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.2328 - acc: 0.9282 - val_loss: 0.2320 - val_acc: 0.9372\n",
      "Epoch 20/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.2260 - acc: 0.9305 - val_loss: 0.2286 - val_acc: 0.9376\n",
      "Epoch 21/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.2200 - acc: 0.9331 - val_loss: 0.2269 - val_acc: 0.9384\n",
      "Epoch 22/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.2157 - acc: 0.9332 - val_loss: 0.2269 - val_acc: 0.9374\n",
      "Epoch 23/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.2116 - acc: 0.9351 - val_loss: 0.2231 - val_acc: 0.9398\n",
      "Epoch 24/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.2087 - acc: 0.9354 - val_loss: 0.2243 - val_acc: 0.9396\n",
      "Epoch 25/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.2031 - acc: 0.9368 - val_loss: 0.2274 - val_acc: 0.9390\n",
      "Epoch 26/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.2046 - acc: 0.9365 - val_loss: 0.2268 - val_acc: 0.9393\n",
      "Epoch 27/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.2009 - acc: 0.9383 - val_loss: 0.2209 - val_acc: 0.9407\n",
      "Epoch 28/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1975 - acc: 0.9391 - val_loss: 0.2208 - val_acc: 0.9409\n",
      "Epoch 29/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1949 - acc: 0.9391 - val_loss: 0.2233 - val_acc: 0.9414\n",
      "Epoch 30/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1931 - acc: 0.9403 - val_loss: 0.2203 - val_acc: 0.9414\n",
      "Epoch 31/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1889 - acc: 0.9410 - val_loss: 0.2219 - val_acc: 0.9417\n",
      "Epoch 32/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1888 - acc: 0.9414 - val_loss: 0.2243 - val_acc: 0.9400\n",
      "Epoch 33/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1873 - acc: 0.9415 - val_loss: 0.2255 - val_acc: 0.9407\n",
      "Epoch 34/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1844 - acc: 0.9426 - val_loss: 0.2224 - val_acc: 0.9398\n",
      "Epoch 35/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1791 - acc: 0.9440 - val_loss: 0.2201 - val_acc: 0.9422\n",
      "Epoch 36/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1796 - acc: 0.9447 - val_loss: 0.2268 - val_acc: 0.9405\n",
      "Epoch 37/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1825 - acc: 0.9432 - val_loss: 0.2181 - val_acc: 0.9420\n",
      "Epoch 38/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1791 - acc: 0.9444 - val_loss: 0.2203 - val_acc: 0.9411\n",
      "Epoch 39/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1741 - acc: 0.9464 - val_loss: 0.2275 - val_acc: 0.9400\n",
      "Epoch 40/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1710 - acc: 0.9467 - val_loss: 0.2206 - val_acc: 0.9431\n",
      "Epoch 41/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1711 - acc: 0.9465 - val_loss: 0.2215 - val_acc: 0.9422\n",
      "Epoch 42/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1715 - acc: 0.9470 - val_loss: 0.2240 - val_acc: 0.9410\n",
      "Epoch 43/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1710 - acc: 0.9468 - val_loss: 0.2219 - val_acc: 0.9425\n",
      "Epoch 44/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1686 - acc: 0.9479 - val_loss: 0.2231 - val_acc: 0.9417\n",
      "Epoch 45/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1659 - acc: 0.9481 - val_loss: 0.2263 - val_acc: 0.9417\n",
      "Epoch 46/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1655 - acc: 0.9486 - val_loss: 0.2250 - val_acc: 0.9421\n",
      "Epoch 47/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1643 - acc: 0.9495 - val_loss: 0.2233 - val_acc: 0.9424\n",
      "Epoch 48/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1633 - acc: 0.9493 - val_loss: 0.2252 - val_acc: 0.9427\n",
      "Epoch 49/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1638 - acc: 0.9502 - val_loss: 0.2192 - val_acc: 0.9436\n",
      "Epoch 50/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1621 - acc: 0.9497 - val_loss: 0.2210 - val_acc: 0.9431\n",
      "Epoch 51/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1599 - acc: 0.9507 - val_loss: 0.2201 - val_acc: 0.9441\n",
      "Epoch 52/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1582 - acc: 0.9509 - val_loss: 0.2210 - val_acc: 0.9441\n",
      "Epoch 53/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1545 - acc: 0.9526 - val_loss: 0.2226 - val_acc: 0.9430\n",
      "Epoch 54/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1546 - acc: 0.9520 - val_loss: 0.2220 - val_acc: 0.9431\n",
      "Epoch 55/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1558 - acc: 0.9526 - val_loss: 0.2239 - val_acc: 0.9434\n",
      "Epoch 56/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1569 - acc: 0.9515 - val_loss: 0.2271 - val_acc: 0.9426\n",
      "Epoch 57/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1554 - acc: 0.9519 - val_loss: 0.2263 - val_acc: 0.9422\n",
      "Epoch 58/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1535 - acc: 0.9527 - val_loss: 0.2281 - val_acc: 0.9410\n",
      "Epoch 59/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1523 - acc: 0.9536 - val_loss: 0.2276 - val_acc: 0.9419\n",
      "Epoch 60/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1528 - acc: 0.9534 - val_loss: 0.2262 - val_acc: 0.9416\n",
      "Epoch 61/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1510 - acc: 0.9538 - val_loss: 0.2279 - val_acc: 0.9420\n",
      "Epoch 62/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1527 - acc: 0.9532 - val_loss: 0.2282 - val_acc: 0.9422\n",
      "Epoch 63/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1511 - acc: 0.9542 - val_loss: 0.2285 - val_acc: 0.9424\n",
      "Epoch 64/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1485 - acc: 0.9543 - val_loss: 0.2256 - val_acc: 0.9436\n",
      "Epoch 65/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1478 - acc: 0.9544 - val_loss: 0.2303 - val_acc: 0.9424\n",
      "Epoch 66/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1471 - acc: 0.9557 - val_loss: 0.2281 - val_acc: 0.9424\n",
      "Epoch 67/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1452 - acc: 0.9556 - val_loss: 0.2281 - val_acc: 0.9430\n",
      "Epoch 68/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1465 - acc: 0.9553 - val_loss: 0.2261 - val_acc: 0.9436\n",
      "Epoch 69/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1480 - acc: 0.9548 - val_loss: 0.2301 - val_acc: 0.9427\n",
      "Epoch 70/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1474 - acc: 0.9553 - val_loss: 0.2271 - val_acc: 0.9434\n",
      "Epoch 71/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1465 - acc: 0.9557 - val_loss: 0.2286 - val_acc: 0.9432\n",
      "Epoch 72/75\n",
      "185892/185892 [==============================] - 3s 15us/step - loss: 0.1415 - acc: 0.9567 - val_loss: 0.2286 - val_acc: 0.9436\n",
      "Epoch 73/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1435 - acc: 0.9564 - val_loss: 0.2272 - val_acc: 0.9440\n",
      "Epoch 74/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1469 - acc: 0.9552 - val_loss: 0.2293 - val_acc: 0.9430\n",
      "Epoch 75/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1458 - acc: 0.9555 - val_loss: 0.2303 - val_acc: 0.9434\n",
      "38547/38547 [==============================] - 2s 44us/step\n",
      "0.8965418839338989\n"
     ]
    }
   ],
   "source": [
    "#2 слоя\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=784, activation='relu'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(49, activation = 'softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train1, y_train1, batch_size=512, epochs=75, validation_split=0.2, verbose=1)\n",
    "test_loss, test_acc = model.evaluate(X_test1, y_test1)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 237459,
     "status": "ok",
     "timestamp": 1576347658068,
     "user": {
      "displayName": "Кирилл Резников",
      "photoUrl": "",
      "userId": "15959288999881424210"
     },
     "user_tz": -120
    },
    "id": "t-iKsBWpFYk1",
    "outputId": "c4243d64-b445-4d3d-8247-4de7ce90db37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 185892 samples, validate on 46473 samples\n",
      "Epoch 1/75\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "185892/185892 [==============================] - 9s 47us/step - loss: 1.0817 - acc: 0.7136 - val_loss: 0.5224 - val_acc: 0.8563\n",
      "Epoch 2/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.5853 - acc: 0.8370 - val_loss: 0.3931 - val_acc: 0.8911\n",
      "Epoch 3/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.4627 - acc: 0.8692 - val_loss: 0.3324 - val_acc: 0.9078\n",
      "Epoch 4/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.3997 - acc: 0.8853 - val_loss: 0.3037 - val_acc: 0.9142\n",
      "Epoch 5/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.3546 - acc: 0.8976 - val_loss: 0.2794 - val_acc: 0.9215\n",
      "Epoch 6/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.3239 - acc: 0.9045 - val_loss: 0.2623 - val_acc: 0.9268\n",
      "Epoch 7/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.3016 - acc: 0.9109 - val_loss: 0.2490 - val_acc: 0.9302\n",
      "Epoch 8/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.2805 - acc: 0.9163 - val_loss: 0.2414 - val_acc: 0.9330\n",
      "Epoch 9/75\n",
      "185892/185892 [==============================] - 3s 17us/step - loss: 0.2639 - acc: 0.9211 - val_loss: 0.2374 - val_acc: 0.9349\n",
      "Epoch 10/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.2535 - acc: 0.9228 - val_loss: 0.2376 - val_acc: 0.9353\n",
      "Epoch 11/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.2429 - acc: 0.9274 - val_loss: 0.2333 - val_acc: 0.9358\n",
      "Epoch 12/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.2275 - acc: 0.9316 - val_loss: 0.2278 - val_acc: 0.9385\n",
      "Epoch 13/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.2213 - acc: 0.9324 - val_loss: 0.2258 - val_acc: 0.9374\n",
      "Epoch 14/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.2152 - acc: 0.9342 - val_loss: 0.2249 - val_acc: 0.9393\n",
      "Epoch 15/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.2063 - acc: 0.9362 - val_loss: 0.2263 - val_acc: 0.9396\n",
      "Epoch 16/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.2009 - acc: 0.9378 - val_loss: 0.2299 - val_acc: 0.9391\n",
      "Epoch 17/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1924 - acc: 0.9404 - val_loss: 0.2209 - val_acc: 0.9416\n",
      "Epoch 18/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1876 - acc: 0.9417 - val_loss: 0.2269 - val_acc: 0.9400\n",
      "Epoch 19/75\n",
      "185892/185892 [==============================] - 3s 17us/step - loss: 0.1868 - acc: 0.9425 - val_loss: 0.2226 - val_acc: 0.9423\n",
      "Epoch 20/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1814 - acc: 0.9433 - val_loss: 0.2234 - val_acc: 0.9413\n",
      "Epoch 21/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1759 - acc: 0.9446 - val_loss: 0.2234 - val_acc: 0.9430\n",
      "Epoch 22/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1715 - acc: 0.9458 - val_loss: 0.2239 - val_acc: 0.9428\n",
      "Epoch 23/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1679 - acc: 0.9474 - val_loss: 0.2220 - val_acc: 0.9441\n",
      "Epoch 24/75\n",
      "185892/185892 [==============================] - 3s 17us/step - loss: 0.1687 - acc: 0.9473 - val_loss: 0.2232 - val_acc: 0.9436\n",
      "Epoch 25/75\n",
      "185892/185892 [==============================] - 3s 17us/step - loss: 0.1646 - acc: 0.9490 - val_loss: 0.2246 - val_acc: 0.9427\n",
      "Epoch 26/75\n",
      "185892/185892 [==============================] - 3s 17us/step - loss: 0.1637 - acc: 0.9488 - val_loss: 0.2222 - val_acc: 0.9454\n",
      "Epoch 27/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1593 - acc: 0.9498 - val_loss: 0.2268 - val_acc: 0.9441\n",
      "Epoch 28/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1558 - acc: 0.9512 - val_loss: 0.2229 - val_acc: 0.9458\n",
      "Epoch 29/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1523 - acc: 0.9523 - val_loss: 0.2335 - val_acc: 0.9431\n",
      "Epoch 30/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1550 - acc: 0.9509 - val_loss: 0.2342 - val_acc: 0.9444\n",
      "Epoch 31/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1491 - acc: 0.9533 - val_loss: 0.2300 - val_acc: 0.9452\n",
      "Epoch 32/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1521 - acc: 0.9524 - val_loss: 0.2332 - val_acc: 0.9445\n",
      "Epoch 33/75\n",
      "185892/185892 [==============================] - 3s 17us/step - loss: 0.1470 - acc: 0.9547 - val_loss: 0.2352 - val_acc: 0.9451\n",
      "Epoch 34/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1456 - acc: 0.9535 - val_loss: 0.2292 - val_acc: 0.9455\n",
      "Epoch 35/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1418 - acc: 0.9552 - val_loss: 0.2346 - val_acc: 0.9444\n",
      "Epoch 36/75\n",
      "185892/185892 [==============================] - 3s 17us/step - loss: 0.1421 - acc: 0.9555 - val_loss: 0.2349 - val_acc: 0.9452\n",
      "Epoch 37/75\n",
      "185892/185892 [==============================] - 3s 17us/step - loss: 0.1399 - acc: 0.9564 - val_loss: 0.2344 - val_acc: 0.9453\n",
      "Epoch 38/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1398 - acc: 0.9568 - val_loss: 0.2371 - val_acc: 0.9450\n",
      "Epoch 39/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1397 - acc: 0.9559 - val_loss: 0.2421 - val_acc: 0.9435\n",
      "Epoch 40/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1374 - acc: 0.9570 - val_loss: 0.2387 - val_acc: 0.9453\n",
      "Epoch 41/75\n",
      "185892/185892 [==============================] - 3s 17us/step - loss: 0.1386 - acc: 0.9567 - val_loss: 0.2357 - val_acc: 0.9458\n",
      "Epoch 42/75\n",
      "185892/185892 [==============================] - 3s 17us/step - loss: 0.1362 - acc: 0.9574 - val_loss: 0.2383 - val_acc: 0.9454\n",
      "Epoch 43/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1323 - acc: 0.9587 - val_loss: 0.2403 - val_acc: 0.9474\n",
      "Epoch 44/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1370 - acc: 0.9576 - val_loss: 0.2368 - val_acc: 0.9473\n",
      "Epoch 45/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1297 - acc: 0.9593 - val_loss: 0.2414 - val_acc: 0.9466\n",
      "Epoch 46/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1327 - acc: 0.9593 - val_loss: 0.2426 - val_acc: 0.9462\n",
      "Epoch 47/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1282 - acc: 0.9602 - val_loss: 0.2414 - val_acc: 0.9467\n",
      "Epoch 48/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1320 - acc: 0.9592 - val_loss: 0.2359 - val_acc: 0.9478\n",
      "Epoch 49/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1283 - acc: 0.9603 - val_loss: 0.2462 - val_acc: 0.9458\n",
      "Epoch 50/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1285 - acc: 0.9603 - val_loss: 0.2485 - val_acc: 0.9466\n",
      "Epoch 51/75\n",
      "185892/185892 [==============================] - 3s 17us/step - loss: 0.1302 - acc: 0.9600 - val_loss: 0.2420 - val_acc: 0.9468\n",
      "Epoch 52/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1257 - acc: 0.9610 - val_loss: 0.2396 - val_acc: 0.9471\n",
      "Epoch 53/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1272 - acc: 0.9607 - val_loss: 0.2503 - val_acc: 0.9467\n",
      "Epoch 54/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1275 - acc: 0.9607 - val_loss: 0.2495 - val_acc: 0.9465\n",
      "Epoch 55/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1229 - acc: 0.9617 - val_loss: 0.2422 - val_acc: 0.9478\n",
      "Epoch 56/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1224 - acc: 0.9623 - val_loss: 0.2450 - val_acc: 0.9468\n",
      "Epoch 57/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1239 - acc: 0.9620 - val_loss: 0.2431 - val_acc: 0.9478\n",
      "Epoch 58/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1184 - acc: 0.9634 - val_loss: 0.2465 - val_acc: 0.9486\n",
      "Epoch 59/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1199 - acc: 0.9627 - val_loss: 0.2489 - val_acc: 0.9478\n",
      "Epoch 60/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1250 - acc: 0.9618 - val_loss: 0.2547 - val_acc: 0.9468\n",
      "Epoch 61/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1231 - acc: 0.9627 - val_loss: 0.2527 - val_acc: 0.9469\n",
      "Epoch 62/75\n",
      "185892/185892 [==============================] - 3s 17us/step - loss: 0.1193 - acc: 0.9639 - val_loss: 0.2506 - val_acc: 0.9473\n",
      "Epoch 63/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1205 - acc: 0.9635 - val_loss: 0.2567 - val_acc: 0.9461\n",
      "Epoch 64/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1195 - acc: 0.9632 - val_loss: 0.2517 - val_acc: 0.9471\n",
      "Epoch 65/75\n",
      "185892/185892 [==============================] - 3s 17us/step - loss: 0.1177 - acc: 0.9641 - val_loss: 0.2509 - val_acc: 0.9473\n",
      "Epoch 66/75\n",
      "185892/185892 [==============================] - 3s 17us/step - loss: 0.1196 - acc: 0.9636 - val_loss: 0.2541 - val_acc: 0.9466\n",
      "Epoch 67/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1182 - acc: 0.9639 - val_loss: 0.2597 - val_acc: 0.9476\n",
      "Epoch 68/75\n",
      "185892/185892 [==============================] - 3s 17us/step - loss: 0.1165 - acc: 0.9646 - val_loss: 0.2591 - val_acc: 0.9474\n",
      "Epoch 69/75\n",
      "185892/185892 [==============================] - 3s 17us/step - loss: 0.1169 - acc: 0.9649 - val_loss: 0.2569 - val_acc: 0.9475\n",
      "Epoch 70/75\n",
      "185892/185892 [==============================] - 3s 18us/step - loss: 0.1124 - acc: 0.9661 - val_loss: 0.2619 - val_acc: 0.9477\n",
      "Epoch 71/75\n",
      "185892/185892 [==============================] - 3s 18us/step - loss: 0.1136 - acc: 0.9653 - val_loss: 0.2583 - val_acc: 0.9478\n",
      "Epoch 72/75\n",
      "185892/185892 [==============================] - 3s 17us/step - loss: 0.1157 - acc: 0.9650 - val_loss: 0.2582 - val_acc: 0.9480\n",
      "Epoch 73/75\n",
      "185892/185892 [==============================] - 3s 17us/step - loss: 0.1129 - acc: 0.9655 - val_loss: 0.2638 - val_acc: 0.9474\n",
      "Epoch 74/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1152 - acc: 0.9654 - val_loss: 0.2588 - val_acc: 0.9478\n",
      "Epoch 75/75\n",
      "185892/185892 [==============================] - 3s 16us/step - loss: 0.1149 - acc: 0.9652 - val_loss: 0.2609 - val_acc: 0.9480\n",
      "38547/38547 [==============================] - 2s 43us/step\n",
      "0.901834124575194\n"
     ]
    }
   ],
   "source": [
    "#3 слоя\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=784, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(49, activation = 'softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train1, y_train1, batch_size=512, epochs=75, validation_split=0.2, verbose=1)\n",
    "test_loss, test_acc = model.evaluate(X_test1, y_test1)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 955
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3826969,
     "status": "ok",
     "timestamp": 1573921603204,
     "user": {
      "displayName": "Кирилл Резников",
      "photoUrl": "",
      "userId": "15959288999881424210"
     },
     "user_tz": -120
    },
    "id": "CsB6l1q1ya9w",
    "outputId": "17db286b-69ae-4331-c1ca-7f007971fce2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Best: 0.974704 using {'dense': 1024, 'dropout_rate': 0.6}\n",
      "0.972061 (0.000781) with: {'dense': 256, 'dropout_rate': 0.3}\n",
      "0.972117 (0.000597) with: {'dense': 256, 'dropout_rate': 0.4}\n",
      "0.971446 (0.001149) with: {'dense': 256, 'dropout_rate': 0.5}\n",
      "0.970908 (0.000311) with: {'dense': 256, 'dropout_rate': 0.6}\n",
      "0.972216 (0.000128) with: {'dense': 512, 'dropout_rate': 0.3}\n",
      "0.973322 (0.000627) with: {'dense': 512, 'dropout_rate': 0.4}\n",
      "0.974028 (0.000589) with: {'dense': 512, 'dropout_rate': 0.5}\n",
      "0.973774 (0.001180) with: {'dense': 512, 'dropout_rate': 0.6}\n",
      "0.972319 (0.000470) with: {'dense': 1024, 'dropout_rate': 0.3}\n",
      "0.972978 (0.000148) with: {'dense': 1024, 'dropout_rate': 0.4}\n",
      "0.974704 (0.000125) with: {'dense': 1024, 'dropout_rate': 0.5}\n",
      "0.974704 (0.000665) with: {'dense': 1024, 'dropout_rate': 0.6}\n"
     ]
    }
   ],
   "source": [
    "def create_model(dropout_rate=0, dense=256):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(28, 28, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(49, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=25, batch_size=512, verbose=0)\n",
    "\n",
    "#batch_size = [64, 128, 256, 512]\n",
    "#epochs = [25, 50, 75, 100]\n",
    "\n",
    "dropout_rate = [0.3, 0.4, 0.5, 0.6]\n",
    "dense = [256, 512, 1024]\n",
    "param_grid = dict(dropout_rate=dropout_rate,dense=dense)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train1, y_train1)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 496896,
     "status": "ok",
     "timestamp": 1573925312445,
     "user": {
      "displayName": "Кирилл Резников",
      "photoUrl": "",
      "userId": "15959288999881424210"
     },
     "user_tz": -120
    },
    "id": "ZSQ-oU7wXQpp",
    "outputId": "a45ecb11-00a6-474a-c19e-6dbc48b6a786"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 185892 samples, validate on 46473 samples\n",
      "Epoch 1/75\n",
      "185892/185892 [==============================] - 7s 39us/step - loss: 0.9457 - acc: 0.7504 - val_loss: 0.2909 - val_acc: 0.9199\n",
      "Epoch 2/75\n",
      "185892/185892 [==============================] - 6s 35us/step - loss: 0.3608 - acc: 0.8983 - val_loss: 0.1927 - val_acc: 0.9466\n",
      "Epoch 3/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.2616 - acc: 0.9248 - val_loss: 0.1570 - val_acc: 0.9575\n",
      "Epoch 4/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.2180 - acc: 0.9370 - val_loss: 0.1402 - val_acc: 0.9619\n",
      "Epoch 5/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.1877 - acc: 0.9457 - val_loss: 0.1295 - val_acc: 0.9643\n",
      "Epoch 6/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.1681 - acc: 0.9511 - val_loss: 0.1199 - val_acc: 0.9667\n",
      "Epoch 7/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.1533 - acc: 0.9553 - val_loss: 0.1134 - val_acc: 0.9694\n",
      "Epoch 8/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.1430 - acc: 0.9578 - val_loss: 0.1163 - val_acc: 0.9678\n",
      "Epoch 9/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.1330 - acc: 0.9601 - val_loss: 0.1045 - val_acc: 0.9716\n",
      "Epoch 10/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.1248 - acc: 0.9628 - val_loss: 0.1073 - val_acc: 0.9706\n",
      "Epoch 11/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.1194 - acc: 0.9636 - val_loss: 0.1025 - val_acc: 0.9723\n",
      "Epoch 12/75\n",
      "185892/185892 [==============================] - 7s 36us/step - loss: 0.1099 - acc: 0.9662 - val_loss: 0.1041 - val_acc: 0.9720\n",
      "Epoch 13/75\n",
      "185892/185892 [==============================] - 7s 36us/step - loss: 0.1073 - acc: 0.9674 - val_loss: 0.0989 - val_acc: 0.9739\n",
      "Epoch 14/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.1024 - acc: 0.9686 - val_loss: 0.1004 - val_acc: 0.9733\n",
      "Epoch 15/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0981 - acc: 0.9695 - val_loss: 0.1006 - val_acc: 0.9737\n",
      "Epoch 16/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0955 - acc: 0.9707 - val_loss: 0.0992 - val_acc: 0.9745\n",
      "Epoch 17/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0909 - acc: 0.9719 - val_loss: 0.0974 - val_acc: 0.9740\n",
      "Epoch 18/75\n",
      "185892/185892 [==============================] - 7s 36us/step - loss: 0.0892 - acc: 0.9723 - val_loss: 0.0989 - val_acc: 0.9743\n",
      "Epoch 19/75\n",
      "185892/185892 [==============================] - 7s 37us/step - loss: 0.0859 - acc: 0.9731 - val_loss: 0.1002 - val_acc: 0.9742\n",
      "Epoch 20/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0835 - acc: 0.9734 - val_loss: 0.0959 - val_acc: 0.9750\n",
      "Epoch 21/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0803 - acc: 0.9747 - val_loss: 0.0967 - val_acc: 0.9762\n",
      "Epoch 22/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0792 - acc: 0.9754 - val_loss: 0.0962 - val_acc: 0.9751\n",
      "Epoch 23/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0761 - acc: 0.9760 - val_loss: 0.0956 - val_acc: 0.9762\n",
      "Epoch 24/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0741 - acc: 0.9767 - val_loss: 0.0964 - val_acc: 0.9754\n",
      "Epoch 25/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0731 - acc: 0.9768 - val_loss: 0.0915 - val_acc: 0.9772\n",
      "Epoch 26/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0706 - acc: 0.9776 - val_loss: 0.0925 - val_acc: 0.9771\n",
      "Epoch 27/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0684 - acc: 0.9783 - val_loss: 0.0924 - val_acc: 0.9766\n",
      "Epoch 28/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0667 - acc: 0.9785 - val_loss: 0.0940 - val_acc: 0.9765\n",
      "Epoch 29/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0671 - acc: 0.9786 - val_loss: 0.0935 - val_acc: 0.9767\n",
      "Epoch 30/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0637 - acc: 0.9795 - val_loss: 0.0960 - val_acc: 0.9762\n",
      "Epoch 31/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0636 - acc: 0.9792 - val_loss: 0.0916 - val_acc: 0.9777\n",
      "Epoch 32/75\n",
      "185892/185892 [==============================] - 7s 36us/step - loss: 0.0644 - acc: 0.9793 - val_loss: 0.0959 - val_acc: 0.9764\n",
      "Epoch 33/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0623 - acc: 0.9798 - val_loss: 0.0937 - val_acc: 0.9763\n",
      "Epoch 34/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0588 - acc: 0.9813 - val_loss: 0.0937 - val_acc: 0.9771\n",
      "Epoch 35/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0574 - acc: 0.9817 - val_loss: 0.0950 - val_acc: 0.9769\n",
      "Epoch 36/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0589 - acc: 0.9808 - val_loss: 0.0935 - val_acc: 0.9775\n",
      "Epoch 37/75\n",
      "185892/185892 [==============================] - 7s 36us/step - loss: 0.0570 - acc: 0.9815 - val_loss: 0.0930 - val_acc: 0.9769\n",
      "Epoch 38/75\n",
      "185892/185892 [==============================] - 7s 37us/step - loss: 0.0583 - acc: 0.9811 - val_loss: 0.0921 - val_acc: 0.9771\n",
      "Epoch 39/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0568 - acc: 0.9820 - val_loss: 0.0915 - val_acc: 0.9779\n",
      "Epoch 40/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0541 - acc: 0.9823 - val_loss: 0.0929 - val_acc: 0.9779\n",
      "Epoch 41/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0549 - acc: 0.9826 - val_loss: 0.0912 - val_acc: 0.9774\n",
      "Epoch 42/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0547 - acc: 0.9826 - val_loss: 0.0952 - val_acc: 0.9769\n",
      "Epoch 43/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0551 - acc: 0.9825 - val_loss: 0.0941 - val_acc: 0.9778\n",
      "Epoch 44/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0511 - acc: 0.9834 - val_loss: 0.0939 - val_acc: 0.9780\n",
      "Epoch 45/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0526 - acc: 0.9832 - val_loss: 0.0939 - val_acc: 0.9785\n",
      "Epoch 46/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0500 - acc: 0.9840 - val_loss: 0.0987 - val_acc: 0.9776\n",
      "Epoch 47/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0509 - acc: 0.9836 - val_loss: 0.0939 - val_acc: 0.9785\n",
      "Epoch 48/75\n",
      "185892/185892 [==============================] - 6s 35us/step - loss: 0.0495 - acc: 0.9839 - val_loss: 0.0954 - val_acc: 0.9781\n",
      "Epoch 49/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0498 - acc: 0.9835 - val_loss: 0.0945 - val_acc: 0.9777\n",
      "Epoch 50/75\n",
      "185892/185892 [==============================] - 6s 35us/step - loss: 0.0504 - acc: 0.9837 - val_loss: 0.0931 - val_acc: 0.9786\n",
      "Epoch 51/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0485 - acc: 0.9842 - val_loss: 0.0948 - val_acc: 0.9779\n",
      "Epoch 52/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0468 - acc: 0.9848 - val_loss: 0.0957 - val_acc: 0.9780\n",
      "Epoch 53/75\n",
      "185892/185892 [==============================] - 6s 35us/step - loss: 0.0469 - acc: 0.9852 - val_loss: 0.0949 - val_acc: 0.9783\n",
      "Epoch 54/75\n",
      "185892/185892 [==============================] - 6s 35us/step - loss: 0.0464 - acc: 0.9847 - val_loss: 0.0948 - val_acc: 0.9780\n",
      "Epoch 55/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0475 - acc: 0.9847 - val_loss: 0.0925 - val_acc: 0.9779\n",
      "Epoch 56/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0455 - acc: 0.9855 - val_loss: 0.0946 - val_acc: 0.9781\n",
      "Epoch 57/75\n",
      "185892/185892 [==============================] - 7s 36us/step - loss: 0.0454 - acc: 0.9851 - val_loss: 0.0969 - val_acc: 0.9777\n",
      "Epoch 58/75\n",
      "185892/185892 [==============================] - 6s 35us/step - loss: 0.0457 - acc: 0.9852 - val_loss: 0.0908 - val_acc: 0.9784\n",
      "Epoch 59/75\n",
      "185892/185892 [==============================] - 7s 36us/step - loss: 0.0441 - acc: 0.9855 - val_loss: 0.0960 - val_acc: 0.9786\n",
      "Epoch 60/75\n",
      "185892/185892 [==============================] - 7s 36us/step - loss: 0.0434 - acc: 0.9859 - val_loss: 0.0970 - val_acc: 0.9779\n",
      "Epoch 61/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0445 - acc: 0.9855 - val_loss: 0.0939 - val_acc: 0.9779\n",
      "Epoch 62/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0432 - acc: 0.9859 - val_loss: 0.0922 - val_acc: 0.9783\n",
      "Epoch 63/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0425 - acc: 0.9863 - val_loss: 0.0955 - val_acc: 0.9784\n",
      "Epoch 64/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0422 - acc: 0.9862 - val_loss: 0.0971 - val_acc: 0.9775\n",
      "Epoch 65/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0437 - acc: 0.9859 - val_loss: 0.0938 - val_acc: 0.9787\n",
      "Epoch 66/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0431 - acc: 0.9860 - val_loss: 0.0934 - val_acc: 0.9789\n",
      "Epoch 67/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0443 - acc: 0.9859 - val_loss: 0.0935 - val_acc: 0.9781\n",
      "Epoch 68/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0421 - acc: 0.9862 - val_loss: 0.0979 - val_acc: 0.9782\n",
      "Epoch 69/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0415 - acc: 0.9864 - val_loss: 0.0942 - val_acc: 0.9789\n",
      "Epoch 70/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0408 - acc: 0.9865 - val_loss: 0.0941 - val_acc: 0.9784\n",
      "Epoch 71/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0406 - acc: 0.9868 - val_loss: 0.0944 - val_acc: 0.9785\n",
      "Epoch 72/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0413 - acc: 0.9867 - val_loss: 0.0992 - val_acc: 0.9780\n",
      "Epoch 73/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0412 - acc: 0.9869 - val_loss: 0.0961 - val_acc: 0.9784\n",
      "Epoch 74/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0389 - acc: 0.9875 - val_loss: 0.0969 - val_acc: 0.9788\n",
      "Epoch 75/75\n",
      "185892/185892 [==============================] - 7s 35us/step - loss: 0.0387 - acc: 0.9870 - val_loss: 0.0955 - val_acc: 0.9789\n",
      "38547/38547 [==============================] - 2s 65us/step\n",
      "0.9627208343061717\n"
     ]
    }
   ],
   "source": [
    "#сверточная сеть\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(28, 28, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(49, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train1, y_train1, batch_size=512, epochs=75, validation_split=0.2, verbose=1)\n",
    "test_loss, test_acc = model.evaluate(X_test1, y_test1)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 657,
     "status": "ok",
     "timestamp": 1573925526451,
     "user": {
      "displayName": "Кирилл Резников",
      "photoUrl": "",
      "userId": "15959288999881424210"
     },
     "user_tz": -120
    },
    "id": "DpoLjrWYODyz",
    "outputId": "e0e2c7e6-c65d-40d8-b1ad-e9b03c4c390e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.plot(history.history['acc'], \n",
    "         label='Доля верных ответов на обучающем наборе')\n",
    "plt.plot(history.history['val_acc'], \n",
    "         label='Доля верных ответов на проверочном наборе')\n",
    "plt.xlabel('Эпоха обучения')\n",
    "plt.ylabel('Доля верных ответов')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YG0VHtzijLsg"
   },
   "outputs": [],
   "source": [
    "model.save('my_model.h5')\n",
    "from google.colab import files\n",
    "files.download('my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "coursachCNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
